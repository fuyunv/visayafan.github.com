#+OPTIONS: ^:{} _:{} num:t toc:t \n:t
#+INCLUDE: "../../layout/template.org"
#+SETUPFILE: "../../layout/extension.org"
#+title:Ubuntu下安装hadoop2.3.0
参考http://raysworld.is-programmer.com/posts/43868.html

* 安装Sun JDK
  首先如果系统中自带OpenJDK，先将其删除：sudo apt-get purge openjdk*
  [[http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html][oracle官网]]下载最新版本的JDK,根据操作系统选择合适的版本（32位的选择jdk-8-linux-i586.tar.gz,64位的选择jdk-8-linux-x64.tar.gz）。
  解压到/usr目录下（可自定义）。
  向/etc/environment最上面添加
  #+BEGIN_EXAMPLE
export JAVA_HOME=/usr/jdk1.8.0
export JRE_HOME=/usr/jdk1.8.0/jre
export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
export CLASSPATH=$CLASSPATH:.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib
  #+END_EXAMPLE
  运行命令java -version如果结果类似：
  #+BEGIN_EXAMPLE
~$ java -version
java version "1.8.0"
Java(TM) SE Runtime Environment (build 1.8.0-b132)
Java HotSpot(TM) Server VM (build 25.0-b70, mixed mode)
  #+END_EXAMPLE
  则表示Sun JDK安装成功。
* 添加hadoop用户
  #+BEGIN_EXAMPLE
  sudo addgroup hadoop
  sudo adduser --ingroup hadoop hadoop  
  #+END_EXAMPLE
  为hadoop用户赋予管理员权限：打开/etc/sudoers，在root ALL=(ALL:ALL) ALL下面一行添加hadoop  ALL=(ALL:ALL) ALL
* 安装SSH
  安装命令：sudo apt-get install openssh-server
  启动SSH服务：sudo /etc/init.d/ssh start
  查看SSH服务是否正确启动:ps -e | grep ssh
  结果类似：
  #+BEGIN_EXAMPLE
  459 ?        00:00:00 sshd
  1327 ?        00:00:00 ssh-agent
  8059 pts/2    00:00:00 ssh
  #+END_EXAMPLE
  表明启动成功。

  配置SSH为无密码登陆：
  #+BEGIN_EXAMPLE
  ssh-keygen -t rsa -P ""
  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
  #+END_EXAMPLE
  之后可用ssh localhost登陆。
* 安装hadoop
  下载最新版本的hadoop（目前是2.3.0），解压到/usr/local/hadoop目录下，此时目录结构应该为：
  #+BEGIN_EXAMPLE
~$ cd /usr/local/hadoop/
/usr/local/hadoop/hadoop$ ls
bin  include  libexec      logs        README.txt  share
etc  lib      LICENSE.txt  NOTICE.txt  sbin
  #+END_EXAMPLE
  下面配置Single Node模式。
  1. 打开etc/hadoop/core-site.xml，修改configuration标签中的内容为：
     #+BEGIN_EXAMPLE
<configuration>
       <property>
               <name>fs.defaultFS</name>
               <value>hdfs://localhost:8020</value>
               <description>The name of the defaultfile system. Either the literal string "local" or a host:port forNDFS.
               </description>
               <final>true</final>
       </property>
</configuration> 
     #+END_EXAMPLE
  2. 打开etc/hadoop/hdfs-site.xml，修改为：
     #+BEGIN_EXAMPLE
<configuration>
     <property>
         <name>dfs.namenode.name.dir</name>
         <value>/home/hadoop/hadoop-2.3.0/dfs/name</value>
     </property>
     <property>
         <name>dfs.datanode.data.dir</name>
         <value>/home/hadoop/hadoop-2.3.0/dfs/data</value>
     </property>
</configuration>     
     #+END_EXAMPLE
  3. 打开etc/hadoop/yarn-site.xml，修改为：
     #+BEGIN_EXAMPLE
<configuration>
<!-- Site specific YARN configuration properties -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>     
     #+END_EXAMPLE
  4. 配置日志输出方式,便于调试错误 export HADOOP_ROOT_LOGGER=INFO,console
  5. 为了避免每次执行hadoop命令都要输入/usr/local/hadoop/bin/hadoop，可以为其定制别名：
     #+BEGIN_EXAMPLE
     alias hadoop=/usr/local/hadoop/bin/hadoop
     #+END_EXAMPLE
     或将hadoop命令添加到PATH中（将下面配置保存到~/.bashrc中）：
     #+BEGIN_EXAMPLE
     export HADOOP_HOME=/usr/local/hadoop
     export PATH=$PATH:$HADOOP_HOME/bin
     #+END_EXAMPLE
     顺便添加export HADOOP_EXAMPLE=$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar减少以后执行命令时敲击键盘次数。
  6. 在家目录/home/hadoop下新建目录hadoop-2.3.0/dfs/name和hadoop-2.3.0/dfs/data，之后格式化文件系统：hadoop namenode -format
  7. 启动服务，切换到/usr/local/hadoop/sbin目录下：
     #+BEGIN_EXAMPLE
./start-dfs.sh
./start-yarn.sh     
     #+END_EXAMPLE
     或直接 ./start-all.sh
  8. 查看状态：hadoop dfsadmin -report或在浏览器中打开http://127.0.0.1:50070/ 查看启动的hadoop服务。
  9. 关闭服务：
     #+BEGIN_EXAMPLE
./stop-dfs.sh     
./stop-yarn.sh     
     #+END_EXAMPLE
     或直接./stop-all.sh
* 测试
  hadoop自带测试用例。
  解压 jar xvf /usr/local/hadoop/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.3.0-sources.jar
** WordCount
   http://wiki.apache.org/hadoop/WordCount
   此用例用来统计指定文件夹下各个单词的出现次数。
   ~/hadoop-2.3.0目录下新建WordCount文件夹并新建file1.txt和file2.txt，内容为：
   #+BEGIN_EXAMPLE
$cat file1.txt 
hello   world
hello   ray
hello   hadoop
$cat file2.txt 
hadoop  ok
hadoop  fail
hadoop  2.3
   #+END_EXAMPLE
   之后将两个文件放到hadoop文件系统中的WordCount目录下：
   #+BEGIN_EXAMPLE
   hadoop fs -copyFromLocal ~/WordCount WordCount
   #+END_EXAMPLE
   运行：
   #+BEGIN_EXAMPLE
   hadoop jar $HADOOP_EXAMPLE wordcount WordCount WordCountOut
   #+END_EXAMPLE
   出现类似下面的结果则表示成功：
   #+BEGIN_EXAMPLE
14/03/31 11:05:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
14/03/31 11:05:23 INFO input.FileInputFormat: Total input paths to process : 2
14/03/31 11:05:24 INFO mapreduce.JobSubmitter: number of splits:2
14/03/31 11:05:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1396229670600_0003
14/03/31 11:05:25 INFO impl.YarnClientImpl: Submitted application application_1396229670600_0003
14/03/31 11:05:25 INFO mapreduce.Job: The url to track the job: http://visayafan-Compaq-510:8088/proxy/application_1396229670600_0003/
14/03/31 11:05:25 INFO mapreduce.Job: Running job: job_1396229670600_0003
14/03/31 11:05:37 INFO mapreduce.Job: Job job_1396229670600_0003 running in uber mode : false
14/03/31 11:05:37 INFO mapreduce.Job:  map 0% reduce 0%
14/03/31 11:06:17 INFO mapreduce.Job:  map 100% reduce 0%
14/03/31 11:06:35 INFO mapreduce.Job:  map 100% reduce 100%
14/03/31 11:06:38 INFO mapreduce.Job: Job job_1396229670600_0003 completed successfully
   #+END_EXAMPLE

   查看WordCountOut下的文件：
   #+BEGIN_EXAMPLE
hadoop$ hadoop fs -ls WordCountOut
Found 2 items
-rw-r--r--   3 hadoop supergroup          0 2014-04-01 16:09 WordCountOut/_SUCCESS
-rw-r--r--   3 hadoop supergroup         49 2014-04-01 16:09 WordCountOut/part-r-00000
   #+END_EXAMPLE
   查看运行结果：
   #+BEGIN_EXAMPLE
$hadoop fs -cat WordCountOut/part-r-00000
2.3	1
hadoop	4
fail	1
hello	3
ok	1
ray	1
world	1
   #+END_EXAMPLE
** pi
   http://thinkinginhadoop.iteye.com/blog/710847
   来用计算pi的值：
   hadoop jar $HADOOP_EXAMPLE pi 1 10
   其中1表示map数，10表示每个map中实验次数(为使结果更精确应该使这两个值尽可能大，但同时耗费的时间也越来越大)。
** grep
   http://wiki.apache.org/hadoop/Grep
   功能：统计输入文件中匹配指定正则表达式字符串及相应的个数。
   在本地系统中创建包含一些文本的文件并复制到HDFS中，假设本地文件为~/Grep/test.txt：
   #+BEGIN_EXAMPLE
   hadoop fs -copyFromLocal ~/Grep Grep
   #+END_EXAMPLE
   之后运行：
   #+BEGIN_EXAMPLE
   hadoop jar $HADOOP_EXAMPLE grep Grep GrepOutput "ou."
   #+END_EXAMPLE
   用来统计以ou为前缀的3个字母构成的字符串，查看结果：
   #+BEGIN_EXAMPLE
   $hadoop fs -cat GrepOutput/part-r-00000
   5	out
   3	oun
   1	oup
   #+END_EXAMPLE
** randomwriter
   用来生成随机数
   #+BEGIN_EXAMPLE
   hadoop jar $HADOOP_EXAMPLE randomwriter rand
   #+END_EXAMPLE
   生成大量随机数保存在rand目录下。
** sort
   首先用randomwirter生成随机数，再用sort排序：
   #+BEGIN_EXAMPLE
   hadoop jar $HADOOP_EXAMPLE sort rand rand_sort
   #+END_EXAMPLE
