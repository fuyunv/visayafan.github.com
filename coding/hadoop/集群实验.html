<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN">
<head>
<title></title>
<!-- 2014-04-03 星期四 10:15 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="generator" content="Org-mode"/>
<meta name="author" content="visayafan"/>
</head>
<body>
<div id="content">
<h1 class="title"></h1>
<div id="table-of-contents">
<h2>&#30446;&#24405;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. 检查实验环境</a></li>
<li><a href="#sec-2">2. 测试自带用例</a>
<ul>
<li><a href="#sec-2-1">2.1. wordcount</a></li>
<li><a href="#sec-2-2">2.2. grep</a></li>
<li><a href="#sec-2-3">2.3. randomwriter</a></li>
<li><a href="#sec-2-4">2.4. sort</a></li>
<li><a href="#sec-2-5">2.5. pi</a></li>
</ul>
</li>
</ul>
</div>
</div>
<!-- <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"> </script> -->
<link rel="stylesheet" type="text/css" href="../../layout/css/bootstrap_old.css" />
<link rel="stylesheet" type="text/css" href="../../layout/css/style.css" />
<script src="../../layout/js/jquery_1.7.1.js"></script>
<script src="../../layout/js/bootstrap_old.js"></script>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> 检查实验环境</h2>
<div class="outline-text-2" id="text-1">
<p>
启动：/usr/sbin/start-all.sh<br/>
结果应为：<br/>
</p>
<pre class="example">
namenode running as process 11301. Stop it first.
192.168.1.15: datanode running as process 14556. Stop it first.
192.168.1.13: datanode running as process 13238. Stop it first.
192.168.1.14: datanode running as process 11585. Stop it first.
192.168.1.18: ssh: connect to host 192.168.1.18 port 22: No route to host
192.168.1.16: ssh: connect to host 192.168.1.16 port 22: No route to host
192.168.1.24: ssh: connect to host 192.168.1.24 port 22: No route to host
192.168.1.23: ssh: connect to host 192.168.1.23 port 22: No route to host
192.168.1.17: ssh: connect to host 192.168.1.17 port 22: No route to host
192.168.1.19: ssh: connect to host 192.168.1.19 port 22: No route to host
192.168.1.21: ssh: connect to host 192.168.1.21 port 22: No route to host
192.168.1.20: ssh: connect to host 192.168.1.20 port 22: No route to host
192.168.1.25: ssh: connect to host 192.168.1.25 port 22: No route to host
192.168.1.22: ssh: connect to host 192.168.1.22 port 22: No route to host
192.168.1.12: secondarynamenode running as process 11536. Stop it first.
jobtracker running as process 11635. Stop it first.
192.168.1.13: tasktracker running as process 13353. Stop it first.
192.168.1.14: tasktracker running as process 11701. Stop it first.
192.168.1.15: tasktracker running as process 14671. Stop it first.
192.168.1.18: ssh: connect to host 192.168.1.18 port 22: No route to host
192.168.1.24: ssh: connect to host 192.168.1.24 port 22: No route to host
192.168.1.16: ssh: connect to host 192.168.1.16 port 22: No route to host
192.168.1.23: ssh: connect to host 192.168.1.23 port 22: No route to host
192.168.1.17: ssh: connect to host 192.168.1.17 port 22: No route to host
192.168.1.21: ssh: connect to host 192.168.1.21 port 22: No route to host
192.168.1.19: ssh: connect to host 192.168.1.19 port 22: No route to host
192.168.1.25: ssh: connect to host 192.168.1.25 port 22: No route to host
192.168.1.20: ssh: connect to host 192.168.1.20 port 22: No route to host
192.168.1.22: ssh: connect to host 192.168.1.22 port 22: No route to host
</pre>
<p>
本次实验环境Node12为master，13/14/15为slaves，其它机器没有启动所以ssh登陆时出现错误。<br/>
启动顺序依次是namenode，3个datanode，secondarynamenode，jobtracker和3个tasktracker。<br/>
</p>

<p>
在master机器上用jps命令查看，结果为：<br/>
</p>
<pre class="example">
25868 Jps
2787 EclipseStarter
11536 SecondaryNameNode
2484 Daemon
11635 JobTracker
11301 NameNode
</pre>
<p>
其中必须有的是JobTracker，Namenode和SecondaryNamenode。<br/>
在slaves机器上结果为：<br/>
</p>
<pre class="example">
[root@node13 ~]# jps
13353 TaskTracker
21867 Jps
13238 DataNode
2397 Daemon
2528 EclipseStarter
</pre>
<p>
其中必须有的是TaskTracker,DataNode。<br/>
否则表明配置有问题。<br/>
</p>

<p>
用hadoop dfsadmin -report查看该集群的状态，结果为：<br/>
</p>
<pre class="example">
Configured Capacity: 421251072000 (392.32 GB)
Present Capacity: 375653658624 (349.85 GB)
DFS Remaining: 375653228544 (349.85 GB)
DFS Used: 430080 (420 KB)
DFS Used%: 0%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Datanodes available: 3 (3 total, 0 dead)

Name: 192.168.1.13:50010
Decommission Status : Normal
Configured Capacity: 140417024000 (130.77 GB)
DFS Used: 143360 (140 KB)
Non DFS Used: 16026447872 (14.93 GB)
DFS Remaining: 124390432768(115.85 GB)
DFS Used%: 0%
DFS Remaining%: 88.59%
Last contact: Thu Apr 03 08:32:10 CST 2014


Name: 192.168.1.14:50010
Decommission Status : Normal
Configured Capacity: 140417024000 (130.77 GB)
DFS Used: 143360 (140 KB)
Non DFS Used: 14807298048 (13.79 GB)
DFS Remaining: 125609582592(116.98 GB)
DFS Used%: 0%
DFS Remaining%: 89.45%
Last contact: Thu Apr 03 08:32:11 CST 2014


Name: 192.168.1.15:50010
Decommission Status : Normal
Configured Capacity: 140417024000 (130.77 GB)
DFS Used: 143360 (140 KB)
Non DFS Used: 14763667456 (13.75 GB)
DFS Remaining: 125653213184(117.02 GB)
DFS Used%: 0%
DFS Remaining%: 89.49%
Last contact: Thu Apr 03 08:32:12 CST 2014
</pre>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> 测试自带用例</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> wordcount</h3>
<div class="outline-text-3" id="text-2-1">
<p>
功能：该用例用于统计指定目录下所有文本中各个单词出现的次数。<br/>
在master机器上新建两个文件内容分别是hello world和hello hadoop的f1.txt和f2.txt并上传到HDFS中：<br/>
</p>
<pre class="example">
[root@node12 fanss]# cd /public/home/fanss/hadooptest/
[root@node12 hadooptest]# mkdir wordcount
[root@node12 hadooptest]# echo "hello world"&gt;wordcount/f1.txt
[root@node12 hadooptest]# echo "hello hadoop"&gt;wordcount/f2.txt
[root@node12 hadooptest]# hadoop fs -copyFromLocal wordcount fan/wordcount
</pre>
<p>
查看是否上传成功：<br/>
</p>
<pre class="example">
[root@node12 hadooptest]# hadoop fs -ls fan/wordcount
Found 2 items
-rw-r--r--   3 root supergroup         12 2014-04-03 08:46 /user/root/fan/wordcount/f1.txt
-rw-r--r--   3 root supergroup         13 2014-04-03 08:46 /user/root/fan/wordcount/f2.txt
</pre>
<p>
执行并将结果保存到fan目录下的wordcountout中：<br/>
</p>
<pre class="example">
[root@node12 hadooptest]# hadoop jar /usr/share/hadoop/hadoop-examples-1.2.1.jar wordcount fan/wordcount fan/wordcountout
14/04/03 08:40:16 INFO input.FileInputFormat: Total input paths to process : 0
14/04/03 08:40:17 INFO mapred.JobClient: Running job: job_201404021741_0003
14/04/03 08:40:18 INFO mapred.JobClient:  map 0% reduce 0%
14/04/03 08:40:26 INFO mapred.JobClient:  map 0% reduce 100%
14/04/03 08:40:27 INFO mapred.JobClient: Job complete: job_201404021741_0003
14/04/03 08:40:27 INFO mapred.JobClient: Counters: 18
14/04/03 08:40:27 INFO mapred.JobClient:   Job Counters
14/04/03 08:40:27 INFO mapred.JobClient:     Launched reduce tasks=1
14/04/03 08:40:27 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=4488
14/04/03 08:40:27 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/04/03 08:40:27 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/04/03 08:40:27 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=4547
14/04/03 08:40:27 INFO mapred.JobClient:   File Output Format Counters
14/04/03 08:40:27 INFO mapred.JobClient:     Bytes Written=0
14/04/03 08:40:27 INFO mapred.JobClient:   FileSystemCounters
14/04/03 08:40:27 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=54298
14/04/03 08:40:27 INFO mapred.JobClient:   Map-Reduce Framework
14/04/03 08:40:27 INFO mapred.JobClient:     Reduce input groups=0
14/04/03 08:40:27 INFO mapred.JobClient:     Combine output records=0
14/04/03 08:40:27 INFO mapred.JobClient:     Reduce shuffle bytes=0
14/04/03 08:40:27 INFO mapred.JobClient:     Physical memory (bytes) snapshot=128315392
14/04/03 08:40:27 INFO mapred.JobClient:     Reduce output records=0
14/04/03 08:40:27 INFO mapred.JobClient:     Spilled Records=0
14/04/03 08:40:27 INFO mapred.JobClient:     CPU time spent (ms)=460
14/04/03 08:40:27 INFO mapred.JobClient:     Total committed heap usage (bytes)=200998912
14/04/03 08:40:27 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1177481216
14/04/03 08:40:27 INFO mapred.JobClient:     Combine input records=0
14/04/03 08:40:27 INFO mapred.JobClient:     Reduce input records=0
</pre>
<p>
查看结果：<br/>
</p>
<pre class="example">
[root@node12 hadooptest]# hadoop fs -cat fan/wordcountout/part-r-00000
hadoop  1
hello   2
world   1
</pre>
<p>
表明hello出现过2次，world和hadoop出现过1次。<br/>
</p>
</div>
</div>

<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> grep</h3>
<div class="outline-text-3" id="text-2-2">
<p>
功能：统计输入文件中匹配指定正则表达式字符串及相应的个数。<br/>
新建文件file.txt向其中随便输入些文件，并上传到HDFS的fan目录下的testgrep中：<br/>
</p>
<pre class="example">
[root@node12 hadooptest]# mkdir testgrep
[root@node12 hadooptest]# echo "The second job takes the output of the first job as input. The mapper is an inverse map, while the reducer is an indentity reducer. The number of reducers is one, so the output is stored in one file, and it is sorted by the count in a descending order. The output file is text, each line of which contains count and a matching string." &gt; testgrep/file.txt
[root@node12 hadooptest]# hadoop fs -copyFromLocal testgrep/ fan/testgrep
</pre>
<p>
执行命令，用来统计以ou为前缀的3个字母构成的字符串的个数：<br/>
</p>
<pre class="example">
[root@node12 hadooptest]# hadoop jar /usr/share/hadoop/hadoop-examples-1.2.1.jar grep  fan/testgrep fan/testgrepout "ou."
14/04/03 08:55:33 INFO util.NativeCodeLoader: Loaded the native-hadoop library
14/04/03 08:55:33 WARN snappy.LoadSnappy: Snappy native library not loaded
14/04/03 08:55:33 INFO mapred.FileInputFormat: Total input paths to process : 1
14/04/03 08:55:34 INFO mapred.JobClient: Running job: job_201404021741_0005
14/04/03 08:55:35 INFO mapred.JobClient:  map 0% reduce 0%
14/04/03 08:55:41 INFO mapred.JobClient:  map 50% reduce 0%
14/04/03 08:55:42 INFO mapred.JobClient:  map 100% reduce 0%
14/04/03 08:55:49 INFO mapred.JobClient:  map 100% reduce 33%
14/04/03 08:55:51 INFO mapred.JobClient:  map 100% reduce 100%
14/04/03 08:55:52 INFO mapred.JobClient: Job complete: job_201404021741_0005
14/04/03 08:55:52 INFO mapred.JobClient: Counters: 30
(部分结果)
</pre>
<p>
结果：<br/>
</p>
<pre class="example">
[root@node12 hadooptest]# hadoop fs -cat fan/testgrepout/part-00000
3       out
2       oun
</pre>
<p>
表明out出现3次，oun出现2次。<br/>
</p>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> randomwriter</h3>
<div class="outline-text-3" id="text-2-3">
<p>
功能：用来生成大量随机数。<br/>
</p>
<pre class="example">
[root@node12 ~]# hadoop jar /usr/share/hadoop/hadoop-examples-1.2.1.jar randomwriter -D test.randomwriter.maps_per_host=1 -D test.randomwrite.bytes_per_map=1024 fan/rand
Running 3 maps.
Job started: Thu Apr 03 09:43:19 CST 2014
14/04/03 09:43:20 INFO mapred.JobClient: Running job: job_201404030924_0002
14/04/03 09:43:21 INFO mapred.JobClient:  map 0% reduce 0%
14/04/03 09:43:28 INFO mapred.JobClient:  map 33% reduce 0%
14/04/03 09:43:29 INFO mapred.JobClient:  map 100% reduce 0%
(部分结果)
14/04/03 09:43:30 INFO mapred.JobClient:     Map input bytes=0
14/04/03 09:43:30 INFO mapred.JobClient:     Map output records=3
14/04/03 09:43:30 INFO mapred.JobClient:     SPLIT_RAW_BYTES=330
Job ended: Thu Apr 03 09:43:30 CST 2014
The job took 10 seconds.
</pre>
<p>
其中test.randomwriter.maps_per_host指定每个节点运行的map数量，默认为10，此处指定为1。test.randomwrite.bytes_per_map指定每个map产生的数据量，默认为1G，此处指定为1k。<br/>
查看fan/rand目录下生成的文件：<br/>
</p>
<pre class="example">
[root@node12 ~]# hadoop fs -ls fan/rand
Found 5 items
-rw-r--r--   3 root supergroup          0 2014-04-03 09:43 /user/root/fan/rand/_SUCCESS
drwxr-xr-x   - root supergroup          0 2014-04-03 09:43 /user/root/fan/rand/_logs
-rw-r--r--   3 root supergroup      11210 2014-04-03 09:43 /user/root/fan/rand/part-00000
-rw-r--r--   3 root supergroup      18964 2014-04-03 09:43 /user/root/fan/rand/part-00001
-rw-r--r--   3 root supergroup       5092 2014-04-03 09:43 /user/root/fan/rand/part-00002
</pre>
</div>
</div>

<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> sort</h3>
<div class="outline-text-3" id="text-2-4">
<p>
功能：用来排序。<br/>
例如对上面randomwriter生成的结果进行排序：<br/>
</p>
<pre class="example">
[root@node12 ~]# hadoop jar /usr/share/hadoop/hadoop-examples-1.2.1.jar sort fan/rand fan/rand_sort
[root@node12 ~]# hadoop jar /usr/share/hadoop/hadoop-examples-1.2.1.jar sort fan/rand fan/rand_sort
Running on 3 nodes to sort from hdfs://192.168.1.12:9000/user/root/fan/rand into hdfs://192.168.1.12:9000/user/root/fan/rand_sort with 5 reduces.
Job started: Thu Apr 03 09:47:45 CST 2014
14/04/03 09:47:45 INFO mapred.FileInputFormat: Total input paths to process : 3
（部分结果）
14/04/03 09:48:05 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=9426591744
14/04/03 09:48:05 INFO mapred.JobClient:     Map output records=3
Job ended: Thu Apr 03 09:48:05 CST 2014
The job took 20 seconds.
</pre>
</div>
</div>

<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5"><span class="section-number-3">2.5</span> pi</h3>
<div class="outline-text-3" id="text-2-5">
<p>
功能：利用Quasi-Monte Carlo算法来估算PI的值。<br/>
</p>
<pre class="example">
[root@node12 ~]# hadoop jar /usr/share/hadoop/hadoop-examples-1.2.1.jar pi 100 100000000
Number of Maps  = 100
Samples per Map = 100000000
（部分结果）
14/04/03 09:54:25 INFO mapred.JobClient:     Reduce input records=200
14/04/03 09:54:25 INFO mapred.JobClient:     Reduce input groups=200
14/04/03 09:54:25 INFO mapred.JobClient:     Combine output records=0
14/04/03 09:54:25 INFO mapred.JobClient:     Physical memory (bytes) snapshot=22874906624
14/04/03 09:54:25 INFO mapred.JobClient:     Reduce output records=0
14/04/03 09:54:25 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=118196387840
14/04/03 09:54:25 INFO mapred.JobClient:     Map output records=200
Job Finished in 157.867 seconds
Estimated value of Pi is 3.14159264920000000000
</pre>
<p>
第1个100指的是要运行100次map任务，第2个数字指的是每个map任务，要投掷多少次，2个参数的乘积就是总的投掷次数。<br/>
</p>
</div>
</div>
</div>
</div>
</body>
</html>
