<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN">
<head>
<title>Ubuntu下安装hadoop2.3.0</title>
<!-- 2014-04-03 星期四 08:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="generator" content="Org-mode"/>
<meta name="author" content="visayafan"/>
</head>
<body>
<div id="content">
<h1 class="title">Ubuntu下安装hadoop2.3.0</h1>
<div id="table-of-contents">
<h2>&#30446;&#24405;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. 安装Sun JDK</a></li>
<li><a href="#sec-2">2. 添加hadoop用户</a></li>
<li><a href="#sec-3">3. 安装SSH</a></li>
<li><a href="#sec-4">4. 安装hadoop</a></li>
<li><a href="#sec-5">5. 测试</a>
<ul>
<li><a href="#sec-5-1">5.1. WordCount</a></li>
<li><a href="#sec-5-2">5.2. pi</a></li>
<li><a href="#sec-5-3">5.3. grep</a></li>
<li><a href="#sec-5-4">5.4. randomwriter</a></li>
<li><a href="#sec-5-5">5.5. sort</a></li>
</ul>
</li>
</ul>
</div>
</div>
<!-- <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"> </script> -->
<link rel="stylesheet" type="text/css" href="../../layout/css/bootstrap_old.css" />
<link rel="stylesheet" type="text/css" href="../../layout/css/style.css" />
<script src="../../layout/js/jquery_1.7.1.js"></script>
<script src="../../layout/js/bootstrap_old.js"></script>
<p>
参考<a href="http://raysworld.is-programmer.com/posts/43868.html">http://raysworld.is-programmer.com/posts/43868.html</a><br/>
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> 安装Sun JDK</h2>
<div class="outline-text-2" id="text-1">
<p>
首先如果系统中自带OpenJDK，先将其删除：sudo apt-get purge openjdk*<br/>
<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">oracle官网</a>下载最新版本的JDK,根据操作系统选择合适的版本（32位的选择jdk-8-linux-i586.tar.gz,64位的选择jdk-8-linux-x64.tar.gz）。<br/>
解压到/usr目录下（可自定义）。<br/>
向/etc/environment最上面添加<br/>
</p>
<pre class="example">
export JAVA_HOME=/usr/jdk1.8.0
export JRE_HOME=/usr/jdk1.8.0/jre
export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
export CLASSPATH=$CLASSPATH:.:$JAVA_HOME/lib:$JAVA_HOME/jre/lib
</pre>
<p>
运行命令java -version如果结果类似：<br/>
</p>
<pre class="example">
~$ java -version
java version "1.8.0"
Java(TM) SE Runtime Environment (build 1.8.0-b132)
Java HotSpot(TM) Server VM (build 25.0-b70, mixed mode)
</pre>
<p>
则表示Sun JDK安装成功。<br/>
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> 添加hadoop用户</h2>
<div class="outline-text-2" id="text-2">
<pre class="example">
sudo addgroup hadoop
sudo adduser --ingroup hadoop hadoop
</pre>
<p>
为hadoop用户赋予管理员权限：打开/etc/sudoers，在root ALL=(ALL:ALL) ALL下面一行添加hadoop  ALL=(ALL:ALL) ALL<br/>
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> 安装SSH</h2>
<div class="outline-text-2" id="text-3">
<p>
安装命令：sudo apt-get install openssh-server<br/>
启动SSH服务：sudo /etc/init.d/ssh start<br/>
查看SSH服务是否正确启动:ps -e | grep ssh<br/>
结果类似：<br/>
</p>
<pre class="example">
459 ?        00:00:00 sshd
1327 ?        00:00:00 ssh-agent
8059 pts/2    00:00:00 ssh
</pre>
<p>
表明启动成功。<br/>
</p>

<p>
配置SSH为无密码登陆：<br/>
</p>
<pre class="example">
ssh-keygen -t rsa -P ""
cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</pre>
<p>
之后可用ssh localhost登陆。<br/>
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> 安装hadoop</h2>
<div class="outline-text-2" id="text-4">
<p>
下载最新版本的hadoop（目前是2.3.0），解压到/usr/local/hadoop目录下，此时目录结构应该为：<br/>
</p>
<pre class="example">
~$ cd /usr/local/hadoop/
/usr/local/hadoop/hadoop$ ls
bin  include  libexec      logs        README.txt  share
etc  lib      LICENSE.txt  NOTICE.txt  sbin
</pre>
<p>
下面配置Single Node模式。<br/>
</p>
<ol class="org-ol">
<li>打开etc/hadoop/core-site.xml，修改configuration标签中的内容为：<br/>
<pre class="example">
&lt;configuration&gt;
       &lt;property&gt;
               &lt;name&gt;fs.defaultFS&lt;/name&gt;
               &lt;value&gt;hdfs://localhost:8020&lt;/value&gt;
               &lt;description&gt;The name of the defaultfile system. Either the literal string "local" or a host:port forNDFS.
               &lt;/description&gt;
               &lt;final&gt;true&lt;/final&gt;
       &lt;/property&gt;
&lt;/configuration&gt;
</pre>
</li>
<li>打开etc/hadoop/hdfs-site.xml，修改为：<br/>
<pre class="example">
&lt;configuration&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
         &lt;value&gt;/home/hadoop/hadoop-2.3.0/dfs/name&lt;/value&gt;
     &lt;/property&gt;
     &lt;property&gt;
         &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
         &lt;value&gt;/home/hadoop/hadoop-2.3.0/dfs/data&lt;/value&gt;
     &lt;/property&gt;
&lt;/configuration&gt;
</pre>
</li>
<li>打开etc/hadoop/yarn-site.xml，修改为：<br/>
<pre class="example">
&lt;configuration&gt;
&lt;!-- Site specific YARN configuration properties --&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
</li>
<li>配置日志输出方式,便于调试错误 export HADOOP_ROOT_LOGGER=INFO,console<br/>
</li>
<li>为了避免每次执行hadoop命令都要输入/usr/local/hadoop/bin/hadoop，可以为其定制别名：<br/>
<pre class="example">
alias hadoop=/usr/local/hadoop/bin/hadoop
</pre>
<p>
或将hadoop命令添加到PATH中（将下面配置保存到~/.bashrc中）：<br/>
</p>
<pre class="example">
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
</pre>
<p>
顺便添加export HADOOP_EXAMPLE=$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar减少以后执行命令时敲击键盘次数。<br/>
</p>
</li>
<li>在家目录/home/hadoop下新建目录hadoop-2.3.0/dfs/name和hadoop-2.3.0/dfs/data，之后格式化文件系统：hadoop namenode -format<br/>
</li>
<li>启动服务，切换到/usr/local/hadoop/sbin目录下：<br/>
<pre class="example">
./start-dfs.sh
./start-yarn.sh
</pre>
<p>
或直接 ./start-all.sh<br/>
</p>
</li>
<li>查看状态：hadoop dfsadmin -report或在浏览器中打开<a href="http://127.0.0.1:50070/">http://127.0.0.1:50070/</a> 查看启动的hadoop服务。<br/>
</li>
<li>关闭服务：<br/>
<pre class="example">
./stop-dfs.sh     
./stop-yarn.sh
</pre>
<p>
或直接./stop-all.sh<br/>
</p>
</li>
</ol>
</div>
</div>

<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> 测试</h2>
<div class="outline-text-2" id="text-5">
<p>
hadoop自带测试用例。<br/>
解压 jar xvf /usr/local/hadoop/share/hadoop/mapreduce/sources/hadoop-mapreduce-examples-2.3.0-sources.jar<br/>
</p>
</div>
<div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1"><span class="section-number-3">5.1</span> WordCount</h3>
<div class="outline-text-3" id="text-5-1">
<p>
<a href="http://wiki.apache.org/hadoop/WordCount">http://wiki.apache.org/hadoop/WordCount</a><br/>
此用例用来统计指定文件夹下各个单词的出现次数。<br/>
~/hadoop-2.3.0目录下新建WordCount文件夹并新建file1.txt和file2.txt，内容为：<br/>
</p>
<pre class="example">
$cat file1.txt 
hello   world
hello   ray
hello   hadoop
$cat file2.txt 
hadoop  ok
hadoop  fail
hadoop  2.3
</pre>
<p>
之后将两个文件放到hadoop文件系统中的WordCount目录下：<br/>
</p>
<pre class="example">
hadoop fs -copyFromLocal ~/WordCount WordCount
</pre>
<p>
运行：<br/>
</p>
<pre class="example">
hadoop jar $HADOOP_EXAMPLE wordcount WordCount WordCountOut
</pre>
<p>
出现类似下面的结果则表示成功：<br/>
</p>
<pre class="example">
14/03/31 11:05:21 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
14/03/31 11:05:23 INFO input.FileInputFormat: Total input paths to process : 2
14/03/31 11:05:24 INFO mapreduce.JobSubmitter: number of splits:2
14/03/31 11:05:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1396229670600_0003
14/03/31 11:05:25 INFO impl.YarnClientImpl: Submitted application application_1396229670600_0003
14/03/31 11:05:25 INFO mapreduce.Job: The url to track the job: http://visayafan-Compaq-510:8088/proxy/application_1396229670600_0003/
14/03/31 11:05:25 INFO mapreduce.Job: Running job: job_1396229670600_0003
14/03/31 11:05:37 INFO mapreduce.Job: Job job_1396229670600_0003 running in uber mode : false
14/03/31 11:05:37 INFO mapreduce.Job:  map 0% reduce 0%
14/03/31 11:06:17 INFO mapreduce.Job:  map 100% reduce 0%
14/03/31 11:06:35 INFO mapreduce.Job:  map 100% reduce 100%
14/03/31 11:06:38 INFO mapreduce.Job: Job job_1396229670600_0003 completed successfully
</pre>

<p>
查看WordCountOut下的文件：<br/>
</p>
<pre class="example">
hadoop$ hadoop fs -ls WordCountOut
Found 2 items
-rw-r--r--   3 hadoop supergroup          0 2014-04-01 16:09 WordCountOut/_SUCCESS
-rw-r--r--   3 hadoop supergroup         49 2014-04-01 16:09 WordCountOut/part-r-00000
</pre>
<p>
查看运行结果：<br/>
</p>
<pre class="example">
$hadoop fs -cat WordCountOut/part-r-00000
2.3 1
hadoop  4
fail    1
hello   3
ok  1
ray 1
world   1
</pre>
</div>
</div>

<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2"><span class="section-number-3">5.2</span> pi</h3>
<div class="outline-text-3" id="text-5-2">
<p>
<a href="http://thinkinginhadoop.iteye.com/blog/710847">http://thinkinginhadoop.iteye.com/blog/710847</a><br/>
来用计算pi的值：<br/>
hadoop jar $HADOOP_EXAMPLE pi 1 10<br/>
其中1表示map数，10表示每个map中实验次数(为使结果更精确应该使这两个值尽可能大，但同时耗费的时间也越来越大)。<br/>
</p>
</div>
</div>

<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3"><span class="section-number-3">5.3</span> grep</h3>
<div class="outline-text-3" id="text-5-3">
<p>
<a href="http://wiki.apache.org/hadoop/Grep">http://wiki.apache.org/hadoop/Grep</a><br/>
功能：统计输入文件中匹配指定正则表达式字符串及相应的个数。<br/>
在本地系统中创建包含一些文本的文件并复制到HDFS中，假设本地文件为~/Grep/test.txt：<br/>
</p>
<pre class="example">
hadoop fs -copyFromLocal ~/Grep Grep
</pre>
<p>
之后运行：<br/>
</p>
<pre class="example">
hadoop jar $HADOOP_EXAMPLE grep Grep GrepOutput "ou."
</pre>
<p>
用来统计以ou为前缀的3个字母构成的字符串，查看结果：<br/>
</p>
<pre class="example">
$hadoop fs -cat GrepOutput/part-r-00000
5    out
3    oun
1    oup
</pre>
</div>
</div>

<div id="outline-container-sec-5-4" class="outline-3">
<h3 id="sec-5-4"><span class="section-number-3">5.4</span> randomwriter</h3>
<div class="outline-text-3" id="text-5-4">
<p>
用来生成随机数<br/>
</p>
<pre class="example">
hadoop jar $HADOOP_EXAMPLE randomwriter rand
</pre>
<p>
生成大量随机数保存在rand目录下。<br/>
</p>
</div>
</div>

<div id="outline-container-sec-5-5" class="outline-3">
<h3 id="sec-5-5"><span class="section-number-3">5.5</span> sort</h3>
<div class="outline-text-3" id="text-5-5">
<p>
首先用randomwirter生成随机数，再用sort排序：<br/>
</p>
<pre class="example">
hadoop jar $HADOOP_EXAMPLE sort rand rand_sort
</pre>
</div>
</div>
</div>
</div>
</body>
</html>
